{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import requests\n",
    "i = 1\n",
    "\n",
    "def get_soup(i):\n",
    "    soup = None\n",
    "    try:\n",
    "        page = requests.get(\"https://www.tiebreak.co.uk/events/search/539253e8756391661f9e033a6dacdd2d/{}\".format(i))\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    except:\n",
    "        pass\n",
    "    return soup\n",
    "\n",
    "def get_soup_url(url):\n",
    "    soup = None\n",
    "    try:\n",
    "        url = \"https:{}\".format(url)\n",
    "        print(url)\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    except:\n",
    "        pass\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_soup_standings(url):\n",
    "    soup = None\n",
    "    try:\n",
    "        url = \"https:{}\".format(url+'/standings')\n",
    "        print(url)\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    except:\n",
    "        pass\n",
    "    return soup    \n",
    "\n",
    "def get_date(i, year):\n",
    "    i = i.contents[0].replace('st ',' ').replace('nd ', ' ').replace('rd ', ' ').replace('th ', ' ')\n",
    "    return datetime.datetime.strptime('{} {}'.format(year, i), '%Y %A %d %b at %I:%M%p')\n",
    "\n",
    "\n",
    "def get_soup_games(url):\n",
    "    soup = None\n",
    "    try:\n",
    "        url = \"https:{}\".format(url+'/feed')\n",
    "        print(url)\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    except:\n",
    "        pass\n",
    "    return soup   \n",
    "\n",
    "\n",
    "def get_soup_url_round(url, i):\n",
    "    soup = None\n",
    "    try:\n",
    "        url = \"https:{}\".format(url+'/round/round-{}'.format(i))\n",
    "        print(url)\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    except:\n",
    "        pass\n",
    "    return soup\n",
    "\n",
    "def get_round_result(soup, date, hours):\n",
    "    result = []\n",
    "    clean_result = []\n",
    "    final_result = []\n",
    "    try:\n",
    "        top = soup.find_all(class_='pairing-top')\n",
    "        for i in top:\n",
    "            result.append([j.contents for j in i.find_all('td')])\n",
    "        bottom = soup.find_all(class_='pairing-bottom')\n",
    "        for i in bottom:\n",
    "            result.append([j.contents for j in i.find_all('td')])\n",
    "        for i in result:\n",
    "            table = int(i[0][0])\n",
    "            name = i[1][0].strip() if len(i[1]) == 2 else i[1][0].contents[0].strip()\n",
    "            guild = i[1][1].contents[0].strip()[1:-1] if len(i[1]) == 2 else i[1][2].contents[0].strip()[1:-1]\n",
    "            home = int(i[2][0] if len(i[1]) == 2 else i[2][0].contents[0])\n",
    "            away = int(i[3][0].contents[0] if len(i[1]) == 2 else i[3][0])\n",
    "            entry = [table, name, guild, home, away]\n",
    "            clean_result.append(entry)\n",
    "        clean_result = sorted(clean_result, key = lambda i: (i[0], -i[3]))\n",
    "        if not isinstance(clean_result[0][0], int):\n",
    "            print(clean_result)\n",
    "        for i in range(int(len(clean_result)/2)):\n",
    "            one = clean_result[i*2]\n",
    "            two = clean_result[i*2+1]\n",
    "            final_result.append([date+datetime.timedelta(seconds = 3600*(11+j)), one[1], one[2], two[1], two[2], one[3], one[4], one[3]==one[4]])\n",
    "    except:\n",
    "        pass\n",
    "    return final_result\n",
    "\n",
    "\n",
    "data= []\n",
    "l=0\n",
    "soup = get_soup(i)\n",
    "while soup.find_all(class_=\"events_listing\"):\n",
    "    for event in soup.find_all(class_=\"events_listing\"):\n",
    "        url = event.find_all(class_='name')[0].a['href']\n",
    "        date = event.find_all(class_='date')[0].find_all('span')[0].contents[0].replace('\\t', '').replace('\\n', '')\n",
    "        if '-' in date:\n",
    "            date = date.split('-')[1]\n",
    "        date = date.replace('st','').replace('nd','').replace('rd','').replace('th','')\n",
    "        date = datetime.datetime.strptime(date, '%d %b %Y')\n",
    "        print(url)\n",
    "        print(date)\n",
    "        j=1\n",
    "        soup2 =  get_soup_standings(url)\n",
    "        try:\n",
    "            players = {a.find_all('a')[0].contents[0]:(b.contents[0] if b.contents else None) for a,b  in zip(*[iter(soup2.find_all(class_='standings-table')[0].find_all(class_='name')[2:])]*2)}\n",
    "            soup2 = get_soup_games(url)\n",
    "            dates = [get_date(i, date.year) for i in soup2.find_all(class_='ts')]\n",
    "            pairings = [i.a.contents[0].split(' beat ') for i in soup2.find_all(class_='winner')]\n",
    "            scores = [i.contents[0].replace('\\n','').replace('F','').replace('A', '').replace('  ',' ').split(':')[1].strip().split(' ')[:2] for i in soup2.find_all(class_='meta') if ':' in i.contents[0]]\n",
    "            result = [[c, a[0], players[a[0]], a[1], players[a[1]]]+b+[False] for a, b, c in zip(pairings, scores, dates) if (len(a) > 1) and a[0] in players and a[1] in players]\n",
    "        except:\n",
    "            print('Not working: {}'.format(url))\n",
    "        data.extend(result)\n",
    "    l += 1\n",
    "    soup = get_soup(l)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "data = [[i[0],i[1],str(i[2]),i[3],str(i[4]),i[5],i[6],i[7]] for i in data if len(i) == 8]\n",
    "pickle.dump( data, open( \"data.pcl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# data= []\n",
    "# i=0\n",
    "# soup = get_soup(i)\n",
    "# while soup:\n",
    "#     for event in soup.find_all(class_=\"events_listing\"):\n",
    "#         url = event.find_all(class_='name')[0].a['href']\n",
    "#         date = event.find_all(class_='date')[0].find_all('span')[0].contents[0].replace('\\t', '').replace('\\n', '')\n",
    "#         if '-' in date:\n",
    "#             date = date.split('-')[1]\n",
    "#         date = date.replace('st','').replace('nd','').replace('rd','').replace('th','')\n",
    "#         date = datetime.datetime.strptime(date, '%d %b %Y')\n",
    "#         print(url)\n",
    "#         print(date)\n",
    "#         j=1\n",
    "#         soup2 =  get_soup_url_round(url, j)\n",
    "#         check = True\n",
    "#         while soup2 and check:\n",
    "#             result = get_round_result(soup2, date, j)\n",
    "#             if not result:\n",
    "#                 check = False\n",
    "#             data.extend(result)\n",
    "#             j+=1\n",
    "#             soup2 =  get_soup_url_round(url, j)\n",
    "#     i += 1\n",
    "#     soup = get_soup(i)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
